from fastapi import FastAPI, UploadFile, File, HTTPException, Form, Depends, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import uvicorn
import os
import json
import logging
import shutil
import tempfile
from datetime import datetime
import uuid
import numpy as np
from pathlib import Path

# Audio processing imports
try:
    import librosa
    import soundfile as sf
    from pydub import AudioSegment
except ImportError:
    logging.warning("Audio processing libraries not installed. Some features may not work.")
    pass

# Initialize logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)

# Initialize FastAPI app
app = FastAPI(
    title="AudioChat API",
    description="Backend API for AudioChat application - Audio Engineering Assistant",
    version="1.0.0",
)

# Create directories for storing audio files
UPLOAD_DIR = Path("uploads")
PROCESSED_DIR = Path("processed")
UPLOAD_DIR.mkdir(exist_ok=True)
PROCESSED_DIR.mkdir(exist_ok=True)

# Mount static files for serving processed audio
app.mount("/audio", StaticFiles(directory="processed"), name="processed_audio")

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, replace with specific origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Models
class Message(BaseModel):
    content: str
    role: str = "user"
    model: Optional[str] = None

class ChatRequest(BaseModel):
    messages: List[Dict[str, Any]]
    model: str
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 1000

class TranscriptionRequest(BaseModel):
    audio_file_path: str
    language: Optional[str] = "en"

class TextToSpeechRequest(BaseModel):
    text: str
    voice: Optional[str] = "default"

# Mock database
conversations = {}
api_keys = {}

# Routes
@app.get("/")
async def root():
    return {"message": "Welcome to AudioChat API"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "timestamp": datetime.now().isoformat()}

# LLM API routes
@app.post("/api/chat")
async def chat(request: ChatRequest):
    """
    Process a chat request with the specified LLM model
    """
    try:
        model = request.model
        logger.info(f"Processing chat request with model: {model}")
        
        # In a real implementation, this would call the appropriate LLM API
        # For now, we'll return a mock response
        response = {
            "id": str(uuid.uuid4()),
            "model": model,
            "created": datetime.now().timestamp(),
            "content": f"This is a mock response from the {model} model. In a real implementation, this would be generated by the LLM API.",
            "role": "assistant"
        }
        
        return response
    except Exception as e:
        logger.error(f"Error processing chat request: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/audio/transcribe")
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Transcribe audio file to text
    """
    try:
        # Save the uploaded file temporarily
        file_path = f"temp_{uuid.uuid4()}.wav"
        with open(file_path, "wb") as f:
            f.write(await file.read())
        
        logger.info(f"Audio file saved to {file_path}")
        
        # In a real implementation, this would call a speech-to-text API
        # For now, we'll return a mock transcription
        transcription = "This is a mock transcription of the uploaded audio file."
        
        # Clean up the temporary file
        os.remove(file_path)
        
        return {"text": transcription}
    except Exception as e:
        logger.error(f"Error transcribing audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/audio/synthesize")
async def text_to_speech(request: TextToSpeechRequest):
    """
    Convert text to speech
    """
    try:
        # In a real implementation, this would call a text-to-speech API
        # For now, we'll return a mock audio URL
        audio_url = f"https://example.com/audio/{uuid.uuid4()}.mp3"
        
        return {"audio_url": audio_url}
    except Exception as e:
        logger.error(f"Error synthesizing speech: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# API key management
@app.post("/api/keys")
async def save_api_key(provider: str = Form(...), key: str = Form(...)):
    """
    Save API key for a provider
    """
    try:
        # In a real implementation, this would securely store the API key
        # For now, we'll just store it in memory
        api_keys[provider] = key
        return {"status": "success", "message": f"API key for {provider} saved successfully"}
    except Exception as e:
        logger.error(f"Error saving API key: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/keys/{provider}")
async def get_api_key(provider: str):
    """
    Get API key for a provider
    """
    if provider not in api_keys:
        raise HTTPException(status_code=404, detail=f"No API key found for {provider}")
    
    # In a real implementation, this would return a masked version of the key
    return {"provider": provider, "key": "********"}

# Conversation management
@app.post("/api/conversations")
async def create_conversation(title: str = Form(...)):
    """
    Create a new conversation
    """
    conversation_id = str(uuid.uuid4())
    conversations[conversation_id] = {
        "id": conversation_id,
        "title": title,
        "created_at": datetime.now().isoformat(),
        "messages": []
    }
    return conversations[conversation_id]

@app.get("/api/conversations")
async def list_conversations():
    """
    List all conversations
    """
    return list(conversations.values())

@app.get("/api/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """
    Get a specific conversation
    """
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail="Conversation not found")
    return conversations[conversation_id]

@app.delete("/api/conversations/{conversation_id}")
async def delete_conversation(conversation_id: str):
    """
    Delete a conversation
    """
    if conversation_id not in conversations:
        raise HTTPException(status_code=404, detail="Conversation not found")
    del conversations[conversation_id]
    return {"status": "success", "message": "Conversation deleted"}

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)#
 Audio processing models
class AudioProcessingRequest(BaseModel):
    file_id: str
    instructions: str
    effects: Optional[List[Dict[str, Any]]] = None

class AudioEffect(BaseModel):
    type: str  # eq, compression, reverb, etc.
    parameters: Dict[str, Any]

# Audio processing functions
def apply_eq(audio_data, sample_rate, parameters):
    """Apply equalization to audio data"""
    try:
        # This is a simplified example - in a real implementation, 
        # we would use proper DSP techniques for equalization
        import librosa.effects as effects
        
        # Extract parameters
        low_shelf = parameters.get('low_shelf', 0)
        low_mid = parameters.get('low_mid', 0)
        mid = parameters.get('mid', 0)
        high_mid = parameters.get('high_mid', 0)
        high = parameters.get('high', 0)
        
        # Apply simple gain adjustments to different frequency bands
        # This is a very simplified approach - real EQ would use filters
        if low_shelf != 0:
            # Apply low shelf EQ (affect frequencies below 250Hz)
            low_mask = librosa.filters.mr_frequencies(sample_rate) < 250
            audio_data[:, low_mask] = audio_data[:, low_mask] * (10 ** (low_shelf / 20))
            
        # Similar processing for other bands...
        
        return audio_data
    except Exception as e:
        logger.error(f"Error applying EQ: {str(e)}")
        return audio_data

def apply_compression(audio_data, parameters):
    """Apply dynamic range compression to audio data"""
    try:
        # Extract parameters
        threshold = parameters.get('threshold', -20)
        ratio = parameters.get('ratio', 4)
        attack = parameters.get('attack', 5)
        release = parameters.get('release', 50)
        
        # Simple compression algorithm
        # This is a simplified version - real compression would be more sophisticated
        threshold_linear = 10 ** (threshold / 20)
        
        # Calculate gain reduction
        gain_reduction = np.zeros_like(audio_data)
        for i in range(len(audio_data)):
            if abs(audio_data[i]) > threshold_linear:
                gain_reduction[i] = abs(audio_data[i]) / threshold_linear
                gain_reduction[i] = gain_reduction[i] ** (1/ratio - 1)
            else:
                gain_reduction[i] = 1.0
                
        # Apply gain reduction
        compressed_audio = audio_data * gain_reduction
        
        return compressed_audio
    except Exception as e:
        logger.error(f"Error applying compression: {str(e)}")
        return audio_data

def apply_reverb(audio_data, sample_rate, parameters):
    """Apply reverb effect to audio data"""
    try:
        # Extract parameters
        room_size = parameters.get('room_size', 0.5)
        damping = parameters.get('damping', 0.5)
        wet_level = parameters.get('wet_level', 0.33)
        dry_level = parameters.get('dry_level', 0.4)
        
        # Simple convolution reverb
        # In a real implementation, we would use a proper reverb algorithm or IR convolution
        reverb_time = int(room_size * sample_rate)
        impulse_response = np.zeros(reverb_time)
        decay = np.linspace(1, 0, reverb_time) ** damping
        impulse_response = decay * np.random.randn(reverb_time)
        
        # Apply convolution
        from scipy import signal
        reverb_audio = signal.convolve(audio_data, impulse_response, mode='full')[:len(audio_data)]
        
        # Mix dry and wet signals
        output = dry_level * audio_data + wet_level * reverb_audio
        
        # Normalize to prevent clipping
        if np.max(np.abs(output)) > 1.0:
            output = output / np.max(np.abs(output))
            
        return output
    except Exception as e:
        logger.error(f"Error applying reverb: {str(e)}")
        return audio_data

# Audio file routes
@app.post("/api/audio/upload")
async def upload_audio(file: UploadFile = File(...)):
    """
    Upload an audio file for processing
    """
    try:
        # Generate a unique ID for the file
        file_id = str(uuid.uuid4())
        file_extension = os.path.splitext(file.filename)[1]
        file_path = UPLOAD_DIR / f"{file_id}{file_extension}"
        
        # Save the uploaded file
        with open(file_path, "wb") as f:
            f.write(await file.read())
        
        logger.info(f"Audio file saved to {file_path}")
        
        # Get basic audio information
        try:
            y, sr = librosa.load(file_path, sr=None)
            duration = librosa.get_duration(y=y, sr=sr)
            
            # Generate waveform data for visualization (downsampled)
            waveform = librosa.resample(y, orig_sr=sr, target_sr=100)
            waveform = waveform[:1000].tolist()  # Limit number of points
            
            audio_info = {
                "file_id": file_id,
                "filename": file.filename,
                "duration": duration,
                "sample_rate": sr,
                "channels": 1 if len(y.shape) == 1 else y.shape[0],
                "waveform": waveform
            }
        except Exception as e:
            logger.error(f"Error analyzing audio: {str(e)}")
            audio_info = {
                "file_id": file_id,
                "filename": file.filename
            }
        
        return audio_info
    except Exception as e:
        logger.error(f"Error uploading audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/audio/process")
async def process_audio(request: AudioProcessingRequest):
    """
    Process an audio file based on natural language instructions
    """
    try:
        # Find the original file
        file_id = request.file_id
        original_files = list(UPLOAD_DIR.glob(f"{file_id}.*"))
        
        if not original_files:
            raise HTTPException(status_code=404, detail="Audio file not found")
        
        original_file = original_files[0]
        file_extension = original_file.suffix
        
        # Load the audio file
        y, sr = librosa.load(original_file, sr=None)
        
        # Process the instructions using LLM to determine which effects to apply
        # For now, we'll use a simplified approach based on keywords
        instructions = request.instructions.lower()
        processed_audio = y.copy()
        
        processing_steps = []
        
        # Apply effects based on instructions
        if "eq" in instructions or "equalization" in instructions or "equalizer" in instructions:
            # Extract EQ parameters from instructions or use defaults
            eq_params = {
                "low_shelf": 3 if "bass" in instructions or "low" in instructions else 0,
                "high": 2 if "treble" in instructions or "high" in instructions else 0
            }
            processed_audio = apply_eq(processed_audio, sr, eq_params)
            processing_steps.append(f"Applied EQ: {'+' if eq_params['low_shelf'] > 0 else ''}{eq_params['low_shelf']}dB low, {'+' if eq_params['high'] > 0 else ''}{eq_params['high']}dB high")
            
        if "compress" in instructions or "compression" in instructions:
            # Extract compression parameters from instructions or use defaults
            comp_params = {
                "threshold": -20,
                "ratio": 4 if "heavy" in instructions else 2
            }
            processed_audio = apply_compression(processed_audio, comp_params)
            processing_steps.append(f"Applied compression: {comp_params['threshold']}dB threshold, {comp_params['ratio']}:1 ratio")
            
        if "reverb" in instructions or "echo" in instructions or "space" in instructions:
            # Extract reverb parameters from instructions or use defaults
            reverb_params = {
                "room_size": 0.8 if "large" in instructions or "hall" in instructions else 0.5,
                "wet_level": 0.5 if "wet" in instructions or "more" in instructions else 0.33
            }
            processed_audio = apply_reverb(processed_audio, sr, reverb_params)
            processing_steps.append(f"Applied reverb: {int(reverb_params['room_size'] * 100)}% room size")
        
        # Save the processed audio
        processed_file_id = str(uuid.uuid4())
        processed_file_path = PROCESSED_DIR / f"{processed_file_id}{file_extension}"
        sf.write(processed_file_path, processed_audio, sr)
        
        # Generate response with processing details
        response = {
            "original_file_id": file_id,
            "processed_file_id": processed_file_id,
            "processing_steps": processing_steps,
            "audio_url": f"/audio/{processed_file_path.name}",
            "instructions": request.instructions
        }
        
        return response
    except Exception as e:
        logger.error(f"Error processing audio: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/audio/{file_id}")
async def get_audio_file(file_id: str):
    """
    Get an audio file by ID
    """
    try:
        # Check processed files first
        processed_files = list(PROCESSED_DIR.glob(f"{file_id}.*"))
        if processed_files:
            return FileResponse(processed_files[0])
            
        # Then check original files
        original_files = list(UPLOAD_DIR.glob(f"{file_id}.*"))
        if original_files:
            return FileResponse(original_files[0])
            
        raise HTTPException(status_code=404, detail="Audio file not found")
    except Exception as e:
        logger.error(f"Error retrieving audio file: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/audio/{file_id}/waveform")
async def get_audio_waveform(file_id: str, points: int = 1000):
    """
    Get waveform data for visualization
    """
    try:
        # Find the file
        processed_files = list(PROCESSED_DIR.glob(f"{file_id}.*"))
        original_files = list(UPLOAD_DIR.glob(f"{file_id}.*"))
        
        file_path = None
        if processed_files:
            file_path = processed_files[0]
        elif original_files:
            file_path = original_files[0]
        else:
            raise HTTPException(status_code=404, detail="Audio file not found")
            
        # Load and downsample the audio
        y, sr = librosa.load(file_path, sr=None)
        
        # Generate waveform data (downsampled)
        waveform = librosa.resample(y, orig_sr=sr, target_sr=points/librosa.get_duration(y=y, sr=sr))
        waveform = waveform[:points].tolist()
        
        return {
            "file_id": file_id,
            "waveform": waveform,
            "sample_rate": sr,
            "duration": librosa.get_duration(y=y, sr=sr)
        }
    except Exception as e:
        logger.error(f"Error generating waveform: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))